{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to for the Sliding Window Algorithm\n",
    "\n",
    "This notebook shows how to load a pre-trained model and run the sliding window algorithm to run object detection on a complex scene containing several objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob, os, sys, inspect\n",
    "import json\n",
    "\n",
    "\n",
    "import models.model_helpers as model_helpers\n",
    "import models.tuning_helpers as tuning_helpers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.data_helpers as data_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model\n",
    "\n",
    "Define where the model is saved and get all parameters.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "\n",
    "# define model path\n",
    "model_path = 'logs/experiments/inception_v3_semiart_2020_09_17-12:14'\n",
    "\n",
    "# load model parameters\n",
    "with open(os.path.join(model_path, 'results_classifier.json')) as fp:\n",
    "    results_classifier = json.load(fp)\n",
    "    \n",
    "classes = results_classifier['classes']\n",
    "ind2class = results_classifier['ind2class']\n",
    "ind2class = {int(k):v for (k,v) in ind2class.items()}\n",
    "\n",
    "# load model\n",
    "loaded_model = load_model(\n",
    "    model_path,\n",
    "    custom_objects=None,\n",
    "    compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    " - First, we load the base dataframe which points to all images in the __validation set__. We have to use the validation set here, to get an unbiased overall score of end-to-end object recognition pipeline.\n",
    " \n",
    "  - For validation, we use a two-fold approach: \n",
    "     - Validation on an artifically generated dataset with randomized objects from the validation set.\n",
    "     - Validation on completely unseend images, realistic fridge scenes which were manually selected before. \n",
    " \n",
    " \n",
    " - We could theoretically, use base dataframe, to generate a new artificial dataset with randomize object positions (this is shown in the commented block). Alternatively, artifical datasets can be created with the script *data/generate_artifical_validation_set.py*\n",
    " \n",
    " - Instead of generating a new artificial dataset here, we just load one which is already contained in the folder *data/validation_artificial*.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation set with \"real\" fridge scenes\n",
    "val_real_data = data_helpers.get_validation_dict('data/validation_real/', classes, verbose=0)\n",
    "\n",
    "# get validation set with \"artificial\" fridge scenes\n",
    "val_artificial_data = data_helpers.get_validation_dict('data/validation_artificial/', classes, verbose=0)\n",
    "\n",
    "# get test set\n",
    "test_data = data_helpers.get_validation_dict('data/test_set/', classes, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATE NEW ARTIFICIAL VALIDATION SET\n",
    "\n",
    "#N_samples = 10\n",
    "#N_min = 5\n",
    "#N_max = 10\n",
    "#spacing = 150\n",
    "#size_jitter=(0.9,1.5)\n",
    "#seed = 11\n",
    "#bg_path = 'data/artificial_background'\n",
    "\n",
    "#val_artificial_data = data_helpers.generate_artifical_validation_dataset(data_df_test, bg_path, N_samples, N_min, N_max, spacing, size_jitter, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show examples pictures\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(val_real_data[15]['image'])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(0.05, 0.98, '\\n'.join(val_real_data[15]['labels']), \n",
    "         transform = ax.transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(val_artificial_data[15]['image'])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.text(0.05, 0.98, '\\n'.join(val_artificial_data[15]['labels']), \n",
    "         transform = ax.transAxes, verticalalignment='top',\n",
    "         bbox=dict(facecolor='red', alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect objects using \"image pyramid\"\n",
    "\n",
    "    - First, generate an \"image pyramid\", i.e. scale the given image by different factors to enable object detection of \n",
    "    differently sized objects. \n",
    "    \n",
    "    - Here, for simplicity, we only use one scaling factor. (So this is not really an image pyramid)\n",
    "    \n",
    "    - Run non-maximum suppression on predicted boxes.\n",
    "    \n",
    "For more info on the \"image pyramid\" see, e.g.:\n",
    "https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_mobilenet_v2\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as preprocess_inception_v3\n",
    "\n",
    "# define correct pre-processing function for the network\n",
    "preprocess_func = preprocess_inception_v3\n",
    "\n",
    "# define correct input size for the network (the one it was trained on)\n",
    "kernel_size = 224\n",
    "\n",
    "# define selection threshold / do not take prediction with a lesser confidence level\n",
    "thr = 0.87\n",
    "\n",
    "# define non-max suppression threshold\n",
    "overlap_thr = 0.2\n",
    "\n",
    "# define image pyramid (object sizes / larger factors correspond to smaller objects)\n",
    "scaling_factors = [1.5]\n",
    "sliding_strides = [64]\n",
    "\n",
    "# select image to perform predictions on\n",
    "#input_sample = val_artificial_data[15]\n",
    "#input_sample = val_real_data[16]\n",
    "input_sample = test_data[35]\n",
    "img = input_sample['image']\n",
    "\n",
    "# perform object detection with final model\n",
    "pred_labels, probabilities, x0, y0, windowsize = \\\n",
    "        model_helpers.object_detection_sliding_window(model=loaded_model, \n",
    "                                                      input_img=img, \n",
    "                                                      preprocess_function=preprocess_func, \n",
    "                                                      kernel_size=kernel_size, \n",
    "                                                      ind2class=ind2class, \n",
    "                                                      scaling_factors=scaling_factors, \n",
    "                                                      sliding_strides=sliding_strides, \n",
    "                                                      thr=thr, \n",
    "                                                      overlap_thr=overlap_thr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = input_sample['labels']\n",
    "\n",
    "# true positives are intersection between predicted and actual labels\n",
    "TP = set(actual_labels).intersection(set(pred_labels))\n",
    "\n",
    "# false positives are difference between predicted and actual labels\n",
    "FP = set(pred_labels) - set(actual_labels)\n",
    "\n",
    "# false negatives are difference between actual and predicted labels\n",
    "FN = set(actual_labels) - set(pred_labels)\n",
    "\n",
    "# true negatives are intersection between the differences between all classes \n",
    "# and the actual and predicted classes respectively\n",
    "# (usually not so important for object detection)\n",
    "TN = (set(classes) - set(actual_labels)).intersection((set(classes) - set(pred_labels)))\n",
    "\n",
    "precision = len(TP) / (len(TP)+len(FP))\n",
    "recall  = len(TP) / (len(TP)+len(FN))\n",
    "\n",
    "print(f'Correctly identified: ' + ', '.join(TP))\n",
    "print(f'Missed items: ' + ', '.join(FN))\n",
    "print(f'Wrong items: ' + ', '.join(FP))\n",
    "print()\n",
    "print(f'Precision: {100*precision:.2f} %')\n",
    "print(f'Recall: {100*recall:.2f} %')\n",
    "\n",
    "# visualize results\n",
    "fig = model_helpers.visualize_predictions(img, \n",
    "                                          pred_labels, \n",
    "                                          probabilities, \n",
    "                                          x0, \n",
    "                                          y0,\n",
    "                                          windowsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "pred_labels, probabilities, x0, y0, windowsize = \\\n",
    "        model_helpers.object_detection_sliding_window(model=loaded_model, \n",
    "                                                      input_img=img, \n",
    "                                                      preprocess_function=preprocess_func, \n",
    "                                                      kernel_size=kernel_size, \n",
    "                                                      ind2class=ind2class, \n",
    "                                                      scaling_factors=scaling_factors, \n",
    "                                                      sliding_strides=sliding_strides, \n",
    "                                                      thr=thr, \n",
    "                                                      overlap_thr=overlap_thr)\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model predictions on whole test set\n",
    "\n",
    "- Iterate over whole test set and make predicitions\n",
    "- Save results / visualizations\n",
    "- Get performance metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for ind in test_data.keys():\n",
    "    \n",
    "    input_sample = test_data[ind]\n",
    "    img = input_sample['image']\n",
    "    \n",
    "    # perform object detection with final model\n",
    "    pred_labels, probabilities, x0, y0, windowsize = \\\n",
    "            model_helpers.object_detection_sliding_window(model=loaded_model, \n",
    "                                                          input_img=img, \n",
    "                                                          preprocess_function=preprocess_func, \n",
    "                                                          kernel_size=kernel_size, \n",
    "                                                          ind2class=ind2class, \n",
    "                                                          scaling_factors=scaling_factors, \n",
    "                                                          sliding_strides=sliding_strides, \n",
    "                                                          thr=thr, \n",
    "                                                          overlap_thr=overlap_thr)\n",
    "    \n",
    "    # get metrics\n",
    "    actual_labels = input_sample['labels']\n",
    "    accuracy, precision, recall, TP, FP, TN, FN = tuning_helpers.get_evaluation_metrics(actual_labels, pred_labels, classes)\n",
    "    \n",
    "    # visualize results\n",
    "    fig = model_helpers.visualize_predictions(img, \n",
    "                                              pred_labels, \n",
    "                                              probabilities, \n",
    "                                              x0, \n",
    "                                              y0,\n",
    "                                              windowsize)\n",
    "    \n",
    "#    # save visualization\n",
    "#    savepath = os.path.join(model_path, 'figures', 'results', 'test')\n",
    "#    if not os.path.isdir(savepath):\n",
    "#        os.makedirs(savepath)\n",
    "#    fig.savefig(os.path.join(savepath, f'img_{ind}.png'), bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # log metrics\n",
    "    log = {'i_img': ind,\n",
    "           'accuracy': accuracy,\n",
    "           'precision': precision,\n",
    "           'recall': recall}\n",
    "            \n",
    "    results.append(log)\n",
    "    \n",
    "\n",
    "#with open(os.path.join(savepath, 'test_resuts.json'), 'w+') as fp:\n",
    "#    json.dump(results, fp)\n",
    "    \n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['f1'] = 2*results_df['precision']*results_df['recall'] / (results_df['precision']+results_df['recall'])\n",
    "results_df.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_food",
   "language": "python",
   "name": "deep_food"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
